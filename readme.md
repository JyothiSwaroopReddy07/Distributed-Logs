# ğŸ§  Distributed Logs System with FastAPI + Kafka + Elasticsearch + Kibana + Redis

A robust, scalable, and efficient logging system using **FastAPI**, **Kafka**, **Elasticsearch**, **Kibana**, and **Redis**. It enables:

- Ingesting logs via an API
- Asynchronous dispatch to Kafka
- Persistence to Elasticsearch
- Visualization with Kibana
- High-performance querying via Redis caching and multithreading
- Fault-tolerant and decoupled log handling

---

## ğŸï¸ Architecture Overview

![Architecture](public/Architecture.png)

---

## ğŸ“… Tech Stack

- **FastAPI**: REST API
- **Kafka**: Queue for log delivery
- **Elasticsearch**: Scalable search engine
- **Kibana**: Visualization tool
- **Redis**: In-memory cache
- **ThreadPoolExecutor**: For multithreaded GET request handling

---

## ğŸª€ Why This Stack?

### âœ… Scalable
- Kafka handles massive ingestion without overloading Elasticsearch.
- Redis minimizes load on Elasticsearch by serving frequent queries.
- Threading optimizes API responsiveness under heavy concurrent reads.

### âš¡ Fast
- Redis gives sub-millisecond access to frequent results.
- Multithreading handles concurrent GET requests efficiently.

### ğŸ›¡ï¸ Reliable
- Kafka decouples ingestion and storage: logs are never lost.
- Cleaner task maintains data integrity by removing old logs.

### ğŸ“Š Efficient
- Async log ingestion with Kafka
- Cached GET queries
- Non-blocking Elasticsearch access via threadpool

---

## ğŸš§ Redis Caching

- Every GET `/logs` request is cached using a hash key of the query params.
- If key is in Redis: return instantly
- If not: fetch from Elasticsearch, then store in Redis for 2 minutes

```python
# Redis GET flow
cache_key = hash(service + start + end)
if redis.exists(cache_key):
    return redis.get(cache_key)
else:
    logs = query_elasticsearch()
    redis.setex(cache_key, logs, ttl=120)
    return logs
```

---

## ğŸªœ Multithreading for Efficiency

GET requests use:
```python
await loop.run_in_executor(executor, function)
```
This offloads CPU-intensive Elasticsearch work to a threadpool, keeping the event loop free for other requests.

---

## ğŸš€ Getting Started

### ğŸ“ Clone
```bash
git clone https://github.com/your-username/distributed-logs.git
cd distributed-logs
```

### ğŸš§ Docker Compose
```bash
docker-compose up -d
```

### ğŸ’¡ Install Python Dependencies
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### ğŸš€ Start FastAPI
```bash
uvicorn app.main:app --reload --port 8001
```

### ğŸš€ Start Kafka Consumer
```bash
python -m app.kafka_consumer
```

---

## ğŸ“Š Kibana Dashboard Setup

1. Go to: [http://localhost:5601](http://localhost:5601)
2. Discover tab â†’ Create index pattern: `logs*`
3. Time filter: `timestamp`

---

## ğŸ“’ Project Structure
```
app/
ğŸ”½ï¸ main.py            # FastAPI API with cache/threading
ğŸ”½ï¸ models.py          # Pydantic model
ğŸ”½ï¸ es_client.py       # Elasticsearch interface
ğŸ”½ï¸ kafka_producer.py  # Kafka producer logic
ğŸ”½ï¸ kafka_consumer.py  # Kafka consumer logic
ğŸ”½ï¸ cleaner.py         # Periodic cleaner
log_generator.py       # Synthetic log poster
log_fetcher.py         # Random GET query tester
requirements.txt
docker-compose.yml
```

---

## ğŸª§ Requirements

```
fastapi==0.110.0
uvicorn[standard]==0.29.0
elasticsearch==8.12.1
python-multipart==0.0.9
requests==2.31.0
pydantic==1.10.13
httpx==0.27.0
kafka-python==2.0.2
urllib3==1.26.18
redis==5.0.1
```

---

## ğŸ“² Synthetic Log Generator
```bash
python log_generator.py
```

## ğŸ¯ Log Fetcher
```bash
python log_fetcher.py
```

---

## ğŸ“Š Sample API Endpoints

### â• POST `/logs`
```json
{
  "timestamp": "2025-03-28T10:00:00",
  "level": "ERROR",
  "message": "Something failed",
  "source": "auth-service"
}
```

### ğŸ” GET `/logs`
```http
/logs?service=backend&start=2025-03-20T00:00:00&end=2025-03-28T00:00:00
```

---

## ğŸ“¸ Screenshots

![Kibana Discover](<public/Screenshot 2025-03-28 at 5.43.44 PM.png>)
![Filters & Timeline](<public/Screenshot 2025-03-28 at 5.44.24 PM.png>)
![Log Visualization](<public/Screenshot 2025-03-28 at 5.44.51 PM.png>)
![Detailed Log View](<public/Screenshot 2025-03-28 at 5.45.29 PM.png>)
![Log Levels](<public/Screenshot 2025-03-28 at 5.45.53 PM.png>)
![Time Explorer](<public/Screenshot 2025-03-28 at 5.52.43 PM.png>)
![Kafka Producer](<public/Screenshot 2025-03-29 at 9.42.11 AM.png>)
![Kafka Consumer](<public/Screenshot 2025-03-29 at 9.42.21 AM.png>)

---

## ğŸš© Summary

This project demonstrates a **production-grade** logging pipeline featuring:

- ğŸªœ **Multithreaded querying**
- ğŸª« **Redis caching**
- âš¡ **Kafka-based ingestion**
- ğŸ“Š **Elastic-powered search**
- ğŸ” **Kibana visual analysis**

Perfect for monitoring, debugging, and log analytics at scale.

---

Feel free to fork, extend, or scale it up further âœ¨

